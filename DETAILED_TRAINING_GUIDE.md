# üéì Complete Training Guide - Dual-Agent UAV RL System

## üìã Training Overview

This guide will walk you through training both agents from scratch to deployment-ready performance. Follow each step carefully for optimal results.

---

## üöÄ Phase 1: Environment Setup & Validation

### Step 1: Verify System Readiness
```bash
# Navigate to project directory
cd c:\Users\burak\Desktop\RL

# Test basic imports and data integrity
python -c "
from ddos_rl.profiles import get_power_consumption, get_crypto_latency
from ddos_rl.env import TacticalUAVEnv
from crypto_rl.strategic_agent import StrategicCryptoEnv
print('‚úÖ All imports successful')
print('‚úÖ Empirical data loaded')
"

# Run comprehensive validation
python validate_system.py
```

### Step 2: Understand the Data Foundation
```bash
# Examine our empirical data
python -c "
from ddos_rl.profiles import *
print('=== POWER SYSTEM ===')
print(f'RPi @ 1200MHz/2-core: {get_rpi_power_only(1200, 2):.2f}W')
print(f'Motors (hover): {get_motor_power(\"hover\"):.0f}W')
print(f'Total: {get_power_consumption(1200, 2):.2f}W')
print()
print('=== CRYPTO LATENCIES ===')
for algo in ['KYBER', 'DILITHIUM', 'SPHINCS', 'FALCON']:
    lat = get_crypto_latency(algo, 1200, 2)
    print(f'{algo}: {lat:.1f}ms @ 1200MHz/2-core')
"
```

---

## üéØ Phase 2: Tactical Agent Training (UAV-Side)

### Understanding the Tactical Agent

**Decision Frequency**: Every 5 seconds  
**State Space**: 144 states (4√ó4√ó3√ó3)  
**Action Space**: 9 actions  
**Goal**: Optimize DDoS detection while preserving battery

#### State Representation
```python
State = [threat_level, battery_level, cpu_load, task_priority]
# threat_level: 0=None, 1=Low, 2=Medium, 3=Critical
# battery_level: 0=Critical(<25%), 1=Low(25-50%), 2=Medium(50-75%), 3=High(75-100%)
# cpu_load: 0=Low(<30%), 1=Medium(30-70%), 2=High(>70%)
# task_priority: 0=Low, 1=Medium, 2=Critical
```

#### Action Space
```python
Actions = {
    0: "XGBOOST @ 600MHz",   1: "XGBOOST @ 1200MHz",
    2: "XGBOOST @ 1800MHz",  3: "XGBOOST @ 2000MHz",
    4: "TST @ 600MHz",       5: "TST @ 1200MHz", 
    6: "TST @ 1800MHz",      7: "TST @ 2000MHz",
    8: "DE_ESCALATE (no scanning)"
}
```

### Step 1: Basic Tactical Training
```bash
# Start with basic training (quick test)
python main.py train tactical --episodes 1000 --save-freq 100 --output tactical_basic.npy

# Monitor training progress
python -c "
import numpy as np
import matplotlib.pyplot as plt

# Load and plot training rewards
try:
    rewards = np.load('tactical_rewards.npy')
    plt.figure(figsize=(10, 6))
    plt.plot(rewards)
    plt.title('Tactical Agent Training Progress')
    plt.xlabel('Episode')
    plt.ylabel('Cumulative Reward')
    plt.grid(True)
    plt.savefig('tactical_training_progress.png')
    print('‚úÖ Training progress saved to tactical_training_progress.png')
except:
    print('‚ö†Ô∏è No training data found yet - run training first')
"
```

### Step 2: Analyze Initial Performance
```bash
# Test the trained agent
python -c "
from ddos_rl.env import TacticalUAVEnv
from ddos_rl.agent import QLearningAgent
import numpy as np

# Load environment and agent
env = TacticalUAVEnv()
agent = QLearningAgent([4,4,3,3], 9)

try:
    agent.load_policy('tactical_basic.npy')
    print('‚úÖ Agent loaded successfully')
    
    # Test agent on different scenarios
    test_scenarios = [
        [0, 3, 0, 0],  # No threat, full battery, low CPU, low priority
        [3, 1, 2, 2],  # Critical threat, low battery, high CPU, critical priority
        [2, 2, 1, 1],  # Medium threat, medium battery, medium CPU, medium priority
    ]
    
    for i, state in enumerate(test_scenarios):
        action = agent.choose_action(state, training=False)
        print(f'Scenario {i+1}: State {state} ‚Üí Action {action}')
        
except FileNotFoundError:
    print('‚ö†Ô∏è No trained model found - run training first')
"
```

### Step 3: Extended Tactical Training
```bash
# Full tactical training with proper hyperparameters
python main.py train tactical \
    --episodes 10000 \
    --learning-rate 0.1 \
    --discount-factor 0.99 \
    --exploration-rate 1.0 \
    --exploration-decay 0.9995 \
    --min-exploration-rate 0.01 \
    --save-freq 1000 \
    --output tactical_full.npy
```

### Step 4: Tactical Training Analysis
```bash
# Analyze Q-table convergence
python -c "
from ddos_rl.agent import QLearningAgent
import numpy as np

agent = QLearningAgent([4,4,3,3], 9)
try:
    agent.load_policy('tactical_full.npy')
    
    print('=== Q-TABLE ANALYSIS ===')
    print(f'Q-table shape: {agent.q_table.shape}')
    print(f'Non-zero entries: {np.count_nonzero(agent.q_table)}/{agent.q_table.size}')
    print(f'State visitation coverage: {np.count_nonzero(agent.state_visits)}/144 states')
    
    # Find most visited states
    top_states = np.argsort(agent.state_visits.flatten())[-10:]
    print('\\nTop 10 most visited states:')
    for idx in reversed(top_states):
        coords = np.unravel_index(idx, agent.state_visits.shape)
        visits = agent.state_visits[coords]
        print(f'  State {list(coords)}: {visits} visits')
    
    # Action distribution
    print('\\nAction distribution:')
    for i, count in enumerate(agent.action_counts):
        print(f'  Action {i}: {count} times ({count/sum(agent.action_counts)*100:.1f}%)')
        
except FileNotFoundError:
    print('‚ö†Ô∏è No trained model found')
"
```

---

## üõ°Ô∏è Phase 3: Strategic Agent Training (GCS-Side)

### Understanding the Strategic Agent

**Decision Frequency**: Every 30 seconds  
**State Space**: 36 states (3√ó3√ó4)  
**Action Space**: 4 actions (crypto algorithms)  
**Goal**: Select optimal cryptographic algorithms for fleet security

#### State Representation
```python
State = [threat_level, avg_battery, mission_phase]
# threat_level: 0=Low, 1=Medium, 2=High
# avg_battery: 0=Low(<40%), 1=Medium(40-70%), 2=High(>70%)
# mission_phase: 0=Takeoff, 1=Transit, 2=Mission, 3=Landing
```

#### Action Space
```python
Actions = {
    0: "KYBER (fast, 8.5/10 security)",
    1: "DILITHIUM (balanced, 9.0/10 security)", 
    2: "SPHINCS (secure, 9.5/10 security)",
    3: "FALCON (balanced, 8.8/10 security)"
}
```

### Step 1: Basic Strategic Training
```bash
# Start strategic agent training
python main.py train strategic --episodes 2000 --save-freq 200 --output strategic_basic.npy
```

### Step 2: Strategic Training Analysis
```bash
# Analyze strategic agent performance
python -c "
from crypto_rl.rl_agent import StrategicQLearningAgent
import numpy as np

agent = StrategicQLearningAgent([3,3,4], 4)
try:
    agent.load_policy('strategic_basic.npy')
    
    print('=== STRATEGIC AGENT ANALYSIS ===')
    print(f'Q-table shape: {agent.q_table.shape}')
    print(f'State coverage: {np.count_nonzero(agent.state_visits)}/36 states')
    
    # Test different scenarios
    test_scenarios = [
        [0, 2, 0],  # Low threat, high battery, takeoff
        [2, 0, 2],  # High threat, low battery, mission
        [1, 1, 3],  # Medium threat, medium battery, landing
    ]
    
    crypto_names = ['KYBER', 'DILITHIUM', 'SPHINCS', 'FALCON']
    
    for i, state in enumerate(test_scenarios):
        action = agent.choose_action(state, training=False)
        print(f'Scenario {i+1}: State {state} ‚Üí {crypto_names[action]}')
        
except FileNotFoundError:
    print('‚ö†Ô∏è No strategic model found - run training first')
"
```

### Step 3: Extended Strategic Training
```bash
# Full strategic training
python main.py train strategic \
    --episodes 5000 \
    --learning-rate 0.05 \
    --discount-factor 0.95 \
    --exploration-rate 0.8 \
    --exploration-decay 0.999 \
    --min-exploration-rate 0.05 \
    --save-freq 500 \
    --output strategic_full.npy
```

---

## üîÑ Phase 4: Integrated Training & Validation

### Step 1: Dual-Agent Integration Test
```bash
# Test both agents working together
python -c "
from ddos_rl.env import TacticalUAVEnv
from crypto_rl.strategic_agent import StrategicCryptoEnv
from ddos_rl.agent import QLearningAgent
from crypto_rl.rl_agent import StrategicQLearningAgent

print('=== DUAL-AGENT INTEGRATION TEST ===')

# Load both agents
tactical_env = TacticalUAVEnv()
strategic_env = StrategicCryptoEnv()

tactical_agent = QLearningAgent([4,4,3,3], 9)
strategic_agent = StrategicQLearningAgent([3,3,4], 4)

try:
    tactical_agent.load_policy('tactical_full.npy')
    strategic_agent.load_policy('strategic_full.npy')
    print('‚úÖ Both agents loaded successfully')
    
    # Simulate coordinated decision making
    print('\\n=== COORDINATED SIMULATION ===')
    
    # Tactical state: [threat, battery, cpu_load, task_priority]
    tactical_state = [2, 1, 1, 2]  # Medium threat, low battery, medium CPU, critical task
    tactical_action = tactical_agent.choose_action(tactical_state, training=False)
    
    # Strategic state: [threat, avg_battery, mission_phase]  
    strategic_state = [2, 1, 2]  # High threat, low battery, mission phase
    strategic_action = strategic_agent.choose_action(strategic_state, training=False)
    
    action_names = ['XGBOOST@600', 'XGBOOST@1200', 'XGBOOST@1800', 'XGBOOST@2000',
                   'TST@600', 'TST@1200', 'TST@1800', 'TST@2000', 'DE_ESCALATE']
    crypto_names = ['KYBER', 'DILITHIUM', 'SPHINCS', 'FALCON']
    
    print(f'Tactical Decision: {action_names[tactical_action]}')
    print(f'Strategic Decision: {crypto_names[strategic_action]}')
    print('‚úÖ Dual-agent coordination successful')
    
except FileNotFoundError as e:
    print(f'‚ö†Ô∏è Missing trained model: {e}')
"
```

### Step 2: Performance Validation
```bash
# Run comprehensive system validation
python tests/test_integration.py

# Generate performance report
python -c "
from ddos_rl.profiles import *
import numpy as np

print('=== SYSTEM PERFORMANCE VALIDATION ===')

# Test power optimization scenarios
configs = [
    (600, 1, 'Low Power'),
    (1200, 2, 'Balanced'), 
    (1800, 4, 'High Performance')
]

for freq, cores, name in configs:
    power = get_power_consumption(freq, cores)
    tst_time = get_ddos_execution_time('TST', freq, cores)
    kyber_lat = get_crypto_latency('KYBER', freq, cores)
    
    print(f'\\n{name} Configuration:')
    print(f'  Power: {power:.1f}W')
    print(f'  TST Execution: {tst_time:.2f}s')
    print(f'  KYBER Latency: {kyber_lat:.1f}ms')
    
    # Flight time calculation
    flight_time_min = 115.44 / power * 60
    print(f'  Flight Time: {flight_time_min:.1f} minutes')

print('\\n=== OPTIMIZATION IMPACT ===')
low_power = get_power_consumption(600, 1)
high_power = get_power_consumption(1800, 4)
time_saved = (115.44/low_power - 115.44/high_power) * 60
print(f'Time saved by optimization: {time_saved:.1f} minutes ({time_saved*60:.0f} seconds)')
"
```

---

## üìä Phase 5: Advanced Training Techniques

### Hyperparameter Tuning
```bash
# Test different learning rates
for lr in 0.05 0.1 0.2; do
    echo "Testing learning rate: $lr"
    python main.py train tactical --episodes 2000 --learning-rate $lr --output tactical_lr_$lr.npy
done

# Compare performance
python -c "
import numpy as np
import matplotlib.pyplot as plt

learning_rates = [0.05, 0.1, 0.2]
colors = ['blue', 'red', 'green']

plt.figure(figsize=(12, 8))

for i, lr in enumerate(learning_rates):
    try:
        rewards = np.load(f'tactical_rewards_lr_{lr}.npy')
        # Smooth rewards with moving average
        window = 100
        smoothed = np.convolve(rewards, np.ones(window)/window, mode='valid')
        plt.plot(smoothed, color=colors[i], label=f'LR={lr}')
    except FileNotFoundError:
        print(f'No data for LR={lr}')

plt.title('Learning Rate Comparison')
plt.xlabel('Episode')
plt.ylabel('Smoothed Reward')
plt.legend()
plt.grid(True)
plt.savefig('learning_rate_comparison.png')
print('‚úÖ Learning rate comparison saved')
"
```

### Exploration Strategy Analysis
```bash
# Test different exploration strategies
python -c "
from ddos_rl.agent import QLearningAgent
import numpy as np

# Compare exploration strategies
strategies = [
    {'decay': 0.999, 'min_rate': 0.01, 'name': 'Conservative'},
    {'decay': 0.9995, 'min_rate': 0.01, 'name': 'Balanced'},
    {'decay': 0.995, 'min_rate': 0.05, 'name': 'Aggressive'}
]

for strategy in strategies:
    print(f'\\n=== {strategy[\"name\"]} Exploration ===')
    
    # Simulate exploration decay
    epsilon = 1.0
    episodes = []
    epsilons = []
    
    for episode in range(10000):
        episodes.append(episode)
        epsilons.append(epsilon)
        epsilon = max(epsilon * strategy['decay'], strategy['min_rate'])
        
        if episode % 2000 == 0:
            print(f'Episode {episode}: epsilon = {epsilon:.4f}')
"
```

---

## üéØ Phase 6: Production Training

### Final Training Configuration
```bash
# Production-ready tactical agent training
python main.py train tactical \
    --episodes 15000 \
    --learning-rate 0.1 \
    --discount-factor 0.99 \
    --exploration-rate 1.0 \
    --exploration-decay 0.9995 \
    --min-exploration-rate 0.01 \
    --save-freq 1000 \
    --validate-freq 2000 \
    --output tactical_production.npy \
    --log-level INFO

# Production-ready strategic agent training  
python main.py train strategic \
    --episodes 8000 \
    --learning-rate 0.05 \
    --discount-factor 0.95 \
    --exploration-rate 0.8 \
    --exploration-decay 0.999 \
    --min-exploration-rate 0.05 \
    --save-freq 500 \
    --validate-freq 1000 \
    --output strategic_production.npy \
    --log-level INFO
```

### Training Monitoring
```bash
# Monitor training in real-time
python -c "
import time
import numpy as np
import matplotlib.pyplot as plt
from matplotlib.animation import FuncAnimation

def monitor_training():
    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 10))
    
    def update_plots(frame):
        try:
            # Load tactical rewards
            tactical_rewards = np.load('tactical_rewards.npy')
            ax1.clear()
            ax1.plot(tactical_rewards[-1000:])  # Last 1000 episodes
            ax1.set_title('Tactical Agent - Recent Performance')
            ax1.set_ylabel('Reward')
            ax1.grid(True)
            
            # Load strategic rewards
            strategic_rewards = np.load('strategic_rewards.npy')
            ax2.clear()
            ax2.plot(strategic_rewards[-500:])  # Last 500 episodes
            ax2.set_title('Strategic Agent - Recent Performance')
            ax2.set_ylabel('Reward')
            ax2.set_xlabel('Episode')
            ax2.grid(True)
            
        except FileNotFoundError:
            pass
    
    ani = FuncAnimation(fig, update_plots, interval=5000)  # Update every 5 seconds
    plt.tight_layout()
    plt.show()

print('Starting training monitor...')
print('Run training in another terminal and watch real-time progress')
# monitor_training()  # Uncomment to run
"
```

---

## üîç Phase 7: Training Evaluation & Optimization

### Performance Metrics
```bash
# Comprehensive performance evaluation
python -c "
from ddos_rl.agent import QLearningAgent
from crypto_rl.rl_agent import StrategicQLearningAgent
import numpy as np

def evaluate_agent_performance():
    print('=== AGENT PERFORMANCE EVALUATION ===')
    
    # Load trained agents
    tactical_agent = QLearningAgent([4,4,3,3], 9)
    strategic_agent = StrategicQLearningAgent([3,3,4], 4)
    
    try:
        tactical_agent.load_policy('tactical_production.npy')
        strategic_agent.load_policy('strategic_production.npy')
        
        print('‚úÖ Production models loaded')
        
        # Tactical agent metrics
        print('\\n=== TACTICAL AGENT METRICS ===')
        print(f'State space coverage: {np.count_nonzero(tactical_agent.state_visits)}/144 ({np.count_nonzero(tactical_agent.state_visits)/144*100:.1f}%)')
        print(f'Q-table density: {np.count_nonzero(tactical_agent.q_table)}/{tactical_agent.q_table.size} ({np.count_nonzero(tactical_agent.q_table)/tactical_agent.q_table.size*100:.1f}%)')
        
        # Action preference analysis
        action_probs = tactical_agent.action_counts / np.sum(tactical_agent.action_counts)
        action_names = ['XGB@600', 'XGB@1200', 'XGB@1800', 'XGB@2000', 'TST@600', 'TST@1200', 'TST@1800', 'TST@2000', 'DE_ESC']
        
        print('\\nAction preferences:')
        for i, (name, prob) in enumerate(zip(action_names, action_probs)):
            print(f'  {name}: {prob*100:.1f}%')
        
        # Strategic agent metrics
        print('\\n=== STRATEGIC AGENT METRICS ===')
        print(f'State space coverage: {np.count_nonzero(strategic_agent.state_visits)}/36 ({np.count_nonzero(strategic_agent.state_visits)/36*100:.1f}%)')
        
        # Crypto algorithm preferences
        crypto_probs = strategic_agent.action_counts / np.sum(strategic_agent.action_counts)
        crypto_names = ['KYBER', 'DILITHIUM', 'SPHINCS', 'FALCON']
        
        print('\\nCrypto algorithm preferences:')
        for name, prob in zip(crypto_names, crypto_probs):
            print(f'  {name}: {prob*100:.1f}%')
            
    except FileNotFoundError as e:
        print(f'‚ö†Ô∏è Model not found: {e}')

evaluate_agent_performance()
"
```

### Policy Analysis
```bash
# Analyze learned policies
python -c "
from ddos_rl.agent import QLearningAgent
import numpy as np

def analyze_tactical_policy():
    agent = QLearningAgent([4,4,3,3], 9)
    agent.load_policy('tactical_production.npy')
    
    print('=== TACTICAL POLICY ANALYSIS ===')
    
    # Analyze policy for different battery levels
    battery_levels = ['Critical', 'Low', 'Medium', 'High']
    action_names = ['XGB@600', 'XGB@1200', 'XGB@1800', 'XGB@2000', 'TST@600', 'TST@1200', 'TST@1800', 'TST@2000', 'DE_ESC']
    
    for battery_idx, battery_name in enumerate(battery_levels):
        print(f'\\n{battery_name} Battery Policy:')
        
        for threat_level in range(4):
            state = [threat_level, battery_idx, 1, 1]  # Medium CPU, medium priority
            action = agent.choose_action(state, training=False)
            print(f'  Threat {threat_level}: {action_names[action]}')

analyze_tactical_policy()
"
```

---

## üöÄ Phase 8: Deployment Preparation

### Model Validation
```bash
# Final validation before deployment
python -c "
from ddos_rl.env import TacticalUAVEnv
from crypto_rl.strategic_agent import StrategicCryptoEnv
from ddos_rl.agent import QLearningAgent
from crypto_rl.rl_agent import StrategicQLearningAgent
import numpy as np

def deployment_validation():
    print('=== DEPLOYMENT VALIDATION ===')
    
    # Load production models
    tactical_agent = QLearningAgent([4,4,3,3], 9)
    strategic_agent = StrategicQLearningAgent([3,3,4], 4)
    
    tactical_agent.load_policy('tactical_production.npy')
    strategic_agent.load_policy('strategic_production.npy')
    
    # Test environments
    tactical_env = TacticalUAVEnv()
    strategic_env = StrategicCryptoEnv()
    
    print('‚úÖ Models and environments loaded')
    
    # Run simulation episodes
    num_episodes = 100
    tactical_rewards = []
    strategic_rewards = []
    
    for episode in range(num_episodes):
        # Tactical episode
        tactical_state = tactical_env.reset()
        tactical_episode_reward = 0
        
        for step in range(100):  # Max 100 steps per episode
            action = tactical_agent.choose_action(tactical_state, training=False)
            next_state, reward, done, _ = tactical_env.step(action)
            tactical_episode_reward += reward
            tactical_state = next_state
            if done:
                break
        
        tactical_rewards.append(tactical_episode_reward)
        
        # Strategic episode (every 6th tactical episode)
        if episode % 6 == 0:
            strategic_state = strategic_env.reset()
            strategic_episode_reward = 0
            
            for step in range(20):  # Max 20 steps per episode
                action = strategic_agent.choose_action(strategic_state, training=False)
                next_state, reward, done, _ = strategic_env.step(action)
                strategic_episode_reward += reward
                strategic_state = next_state
                if done:
                    break
            
            strategic_rewards.append(strategic_episode_reward)
    
    # Performance summary
    print(f'\\n=== VALIDATION RESULTS ===')
    print(f'Tactical Agent:')
    print(f'  Average Reward: {np.mean(tactical_rewards):.2f} ¬± {np.std(tactical_rewards):.2f}')
    print(f'  Best Episode: {np.max(tactical_rewards):.2f}')
    print(f'  Worst Episode: {np.min(tactical_rewards):.2f}')
    
    print(f'\\nStrategic Agent:')
    print(f'  Average Reward: {np.mean(strategic_rewards):.2f} ¬± {np.std(strategic_rewards):.2f}')
    print(f'  Best Episode: {np.max(strategic_rewards):.2f}')
    print(f'  Worst Episode: {np.min(strategic_rewards):.2f}')
    
    # Deployment readiness check
    tactical_ready = np.mean(tactical_rewards) > 0
    strategic_ready = np.mean(strategic_rewards) > 0
    
    if tactical_ready and strategic_ready:
        print('\\n‚úÖ SYSTEM READY FOR DEPLOYMENT')
    else:
        print('\\n‚ö†Ô∏è ADDITIONAL TRAINING RECOMMENDED')
        if not tactical_ready:
            print('   - Tactical agent needs improvement')
        if not strategic_ready:
            print('   - Strategic agent needs improvement')

deployment_validation()
"
```

### Save Production Models
```bash
# Create production model package
mkdir -p models/production
cp tactical_production.npy models/production/
cp strategic_production.npy models/production/

# Create model metadata
python -c "
import json
import numpy as np
from datetime import datetime

metadata = {
    'training_date': datetime.now().isoformat(),
    'tactical_model': {
        'episodes': 15000,
        'learning_rate': 0.1,
        'discount_factor': 0.99,
        'exploration_decay': 0.9995,
        'state_space': [4, 4, 3, 3],
        'action_space': 9
    },
    'strategic_model': {
        'episodes': 8000,
        'learning_rate': 0.05,
        'discount_factor': 0.95,
        'exploration_decay': 0.999,
        'state_space': [3, 3, 4],
        'action_space': 4
    },
    'validation_passed': True,
    'deployment_ready': True
}

with open('models/production/metadata.json', 'w') as f:
    json.dump(metadata, f, indent=2)

print('‚úÖ Production models packaged with metadata')
"
```

---

## üìà Training Success Metrics

### What to Look For:

#### **Tactical Agent Success Indicators:**
- **Reward Convergence**: Rewards should stabilize after 8000-10000 episodes
- **State Coverage**: >80% of state space visited
- **Action Distribution**: Balanced use of different CPU frequencies
- **Battery Preservation**: Preference for lower power modes when threat is low

#### **Strategic Agent Success Indicators:**
- **Faster Convergence**: Should stabilize after 4000-5000 episodes
- **Security Adaptation**: Higher security algorithms for high threat scenarios
- **Battery Awareness**: Faster algorithms when fleet battery is low
- **Mission Phase Adaptation**: Different strategies for different flight phases

#### **Integration Success Indicators:**
- **Coordination**: Agents make complementary decisions
- **Real-time Performance**: Decisions within time constraints (5s tactical, 30s strategic)
- **Power Optimization**: System achieves 46+ seconds flight time improvement
- **Security Maintenance**: Maintains >8.5/10 average security rating

---

## üéØ Training Complete!

After following this guide, you'll have:
- ‚úÖ **Trained tactical agent** optimizing DDoS detection and power usage
- ‚úÖ **Trained strategic agent** selecting optimal cryptographic algorithms  
- ‚úÖ **Validated system integration** with dual-agent coordination
- ‚úÖ **Production-ready models** with comprehensive performance metrics
- ‚úÖ **Deployment package** ready for real hardware testing

Your system is now ready to save those critical 46 seconds of flight time that could save lives! üöÅ‚ú®

**Next Step**: Deploy to actual hardware using `python main.py deploy --mode production`
