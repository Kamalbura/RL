Project Progress Log
Date: 2025-08-23

Summary
- Tactical RL environment refactored to 9-action matrix (XGBOOST/TST × CPU presets + DE-ESCALATE) with Wh-based battery model grounded in performance profiles.
- Conda environment rl_env set up; PyTorch (CUDA 12.1) installed; GPU detected: NVIDIA GeForce RTX 2050.
- Core Python deps installed via conda (gymnasium, matplotlib, tqdm). 
- Tactical agent trained (smoke + long run) and evaluated; UAVScheduler demo updated to load trained Q-table and decode actions.
- Strategic RL work staged (simulator present) — training and demo pending.

Environment
- Conda env: rl_env (Python 3.10.18)
- PyTorch: 2.5.1, CUDA available: True, Device: NVIDIA GeForce RTX 2050
- Installed packages (conda): gymnasium, matplotlib, tqdm

Artifacts
- output_smoke/tactical_q_table_best.npy (updated after 5k episodes)
- output_smoke/tactical_q_table.npy
- output_smoke/tactical_training_curves.png, output_smoke/tactical_epsilon.png
- UAVScheduler.py (updated to new action/state mapping and readable action decoding)
- TRAINING_GUIDE.txt (how to train/evaluate)

Tactical RL (current metrics)
- Evaluation (100 episodes):
  - Before long training: avg reward ≈ 3793.11
  - After 5k episodes: avg reward ≈ 5113.49
- Demo sample outputs show actions like XGBOOST@POWERSAVE/ TURBO depending on state.

How to run
- Demo: python UAVScheduler.py
- Short training smoke: python -c "from train_tactical import train_tactical_agent; train_tactical_agent(episodes=100, eval_frequency=20, output_dir='output_smoke')"

Open items / Next steps
- Tactical: optionally extend training to 20k episodes for stability; consider reward normalization/clipping.
- Strategic: align strategic_simulator with LATENCY_PROFILES; train strategic agent; add a StrategicScheduler demo.
- Reproducibility: add environment.yml capturing rl_env packages.
- Tests: add quick unit tests for action decoding and environment reset/step invariants.

Requirements coverage
- Tactical model for power-aware DDoS model choice: Done (trained baseline, demo wired).
- Strategic model for crypto selection: In progress (simulator exists; training/demo pending).
- Empirical, table-driven calculations: Done (performance_profiles.py used by simulators).
- UAVScheduler demo loads trained tactical Q-table and simulates states: Done.
- MQTT/integration changes: Deferred per scope.

Date: 2025-08-24

Summary
- ddos_rl is now fully self-contained; env fixed (render) and uses local config.
- Implemented GCS-side Strategic Crypto RL (state [Threat, AvgBattery, MissionPhase], 4 crypto actions) with training/eval helpers.
- Added coordination protocol (JSON messages), swarm consensus manager, and a minimal validation scaffold (baseline vs RL) for crypto.
- Added unit tests for strategic agent, coordination messages, and consensus logic.
- Ran smoke training for strategic agent; saved policy.

New Artifacts / Files
- output_smoke/strategic_crypto_q_table.npy (strategic RL smoke policy)
- crypto_rl/strategic_agent.py (env+agent+train/evaluate)
- crypto_rl/train_strategic.py (runner)
- crypto_rl/coordination.py (GCS↔drone JSON protocol)
- crypto_rl/swarm_consensus.py (Byzantine-tolerant threat consensus)
- crypto_rl/validation.py (baseline vs RL comparison for crypto)
- crypto_rl/tests/test_strategic_agent.py, test_coordination_and_consensus.py

How to run
- Strategic RL quick train: python crypto_rl/train_strategic.py
- Crypto scheduler demo: python crypto_rl/crypto_scheduler.py
- Validation (baseline vs RL): use crypto_rl/validation.compare_baseline_vs_rl()
- Tests: pip install pytest; python -m pytest -q

Tactical RL (status unchanged today)
- Best Q-table already saved under output_smoke; UAVScheduler demo works with 9-action mapping.

Open items / Next steps
- Integrate StrategicCryptoAgent decisions into GCS MQTT scheduler using coordination messages.
- Extend validation to joint crypto+tactical scenarios and add plots.
- Export environment.yml for reproducibility.
- Optional longer tactical training (e.g., 20k episodes); add more unit tests for ddos env invariants.

Requirements coverage (delta)
- Tactical DDoS agent: Done (no change).
- Strategic crypto RL agent: Implemented (training+policy save). Integration into GCS UI/MQTT: Pending.
- Coordination protocol: Done.
- Swarm consensus: Done.
- Validation scaffold: Done (crypto baseline vs RL). Broader scenarios: Pending.
