# ===============================================================================
# DDOS-RL COMPLETE CODEBASE BACKUP
# ===============================================================================
# DDOS-RL MODULE BACKUP - Complete Python Scripts Consolidation
# Enhanced with Thermal-Aware RL and Resource Monitoring
# Generated for comprehensive backup and understanding
# ===============================================================================

# ===============================================================================
# FILE: ddos_rl/UAVScheduler.py
# ===============================================================================

import os
from .agent import QLearningAgent
from .profiles import CPU_FREQUENCY_PRESETS

QTABLE_CANDIDATES = [
    os.path.join("output_smoke", "tactical_q_table_best.npy"),
    os.path.join("output_smoke", "tactical_q_table.npy"),
    "tactical_q_table_best.npy",
    "tactical_q_table.npy",
]


class UAVScheduler:
    def __init__(self, qtable_path: str | None = None):
        self.state_dims = [4, 4, 3, 3]
        self.action_dim = 9
        self.agent = QLearningAgent(self.state_dims, self.action_dim)

        path = qtable_path
        if path is None:
            for cand in QTABLE_CANDIDATES:
                if os.path.exists(cand):
                    path = cand
                    break
        if path is None:
            raise FileNotFoundError(
                "No trained tactical Q-table found. Run train_tactical.py to generate one."
            )

        self.agent.load_policy(path)
        print(f"Using Q-table: {path}")

        self._freq_keys = list(CPU_FREQUENCY_PRESETS.keys())

        self._scenarios = [
            [0, 3, 0, 2],
            [1, 2, 1, 1],
            [2, 1, 2, 0],
            [3, 0, 2, 0],
            [1, 3, 1, 2],
        ]
        self._scenario_idx = 0

    def _decode_action(self, action: int) -> str:
        if action == 8:
            return "DE-ESCALATE"
        model = "XGBOOST" if (action // 4) == 0 else "TST"
        freq_idx = action % 4
        preset = self._freq_keys[freq_idx]
        return f"{model}@{preset}"

    def _get_simulated_state(self):
        state = self._scenarios[self._scenario_idx % len(self._scenarios)]
        self._scenario_idx += 1
        return state

    def _manage_tactical_policy_rl(self, state):
        action = self.agent.choose_action(state, training=False)
        readable = self._decode_action(action)
        print(f"[Policy] State {state} -> Action {action} ({readable})")
        return readable

    def run(self, steps=5):
        print("=== UAVScheduler (Tactical) Demo ===")
        print("State format: [Threat, Battery, CPU Load, Task Priority]")
        for i in range(steps):
            print(f"\n--- Step {i+1} ---")
            state = self._get_simulated_state()
            self._manage_tactical_policy_rl(state)


if __name__ == "__main__":
    scheduler = UAVScheduler()
    scheduler.run(steps=5)

# ===============================================================================
# FILE: ddos_rl/agent.py
# ===============================================================================

"""
QLearningAgent (moved from top-level rl_agent.py)
"""

import numpy as np
import os
from typing import List, Tuple, Dict, Any


class QLearningAgent:
	"""
	Q-Learning agent for UAV cybersecurity decision making
	"""

	def __init__(self, state_dims: List[int], action_dim: int, learning_rate: float = 0.1,
				 discount_factor: float = 0.99, exploration_rate: float = 1.0,
				 exploration_decay: float = 0.9995, min_exploration_rate: float = 0.01):
		self.state_dims = state_dims
		self.action_dim = action_dim
		self.learning_rate = learning_rate
		self.discount_factor = discount_factor
		self.epsilon = exploration_rate
		self.epsilon_decay = exploration_decay
		self.min_epsilon = min_exploration_rate

		# Initialize Q-table
		self.q_table = np.zeros(state_dims + [action_dim])

		# Training metrics
		self.training_episodes = 0
		self.training_steps = 0
		
		# State visitation tracking
		self.state_visits = np.zeros(state_dims)
		self.action_counts = np.zeros(action_dim)

	def _state_to_index(self, state: List[int]) -> Tuple[int, ...]:
		return tuple(state)

	def choose_action(self, state: List[int], training: bool = True) -> int:
		state_index = self._state_to_index(state)
		
		# Track state visitation during training
		if training:
			self.state_visits[state_index] += 1
		
		if training and np.random.random() < self.epsilon:
			action = np.random.randint(self.action_dim)
		else:
			action = int(np.argmax(self.q_table[state_index]))
		
		# Track action selection
		if training:
			self.action_counts[action] += 1
			
		return action

	def learn(self, state: List[int], action: int, reward: float, next_state: List[int], done: bool) -> None:
		state_index = self._state_to_index(state)
		next_state_index = self._state_to_index(next_state)
		current_q = self.q_table[state_index][action]
		max_next_q = np.max(self.q_table[next_state_index]) if not done else 0
		new_q = current_q + self.learning_rate * (reward + self.discount_factor * max_next_q - current_q)
		self.q_table[state_index][action] = new_q
		if self.epsilon > self.min_epsilon:
			self.epsilon *= self.epsilon_decay
		self.training_steps += 1

	def save_policy(self, filepath: str) -> None:
		os.makedirs(os.path.dirname(os.path.abspath(filepath)), exist_ok=True)
		np.save(filepath, self.q_table)
		print(f"Policy saved to {filepath}")

	def load_policy(self, filepath: str) -> bool:
		if os.path.exists(filepath):
			self.q_table = np.load(filepath)
			print(f"Policy loaded from {filepath}")
			return True
		else:
			print(f"Policy file {filepath} not found")
			return False

	def get_best_action_for_state(self, state: List[int]) -> Tuple[int, float]:
		state_index = self._state_to_index(state)
		action = int(np.argmax(self.q_table[state_index]))
		q_value = float(self.q_table[state_index][action])
		return action, q_value

	def get_policy_summary(self) -> Dict[str, Any]:
		num_states = int(np.prod(self.state_dims))
		num_nonzero = int(np.count_nonzero(self.q_table))
		coverage = (num_nonzero / (num_states * self.action_dim)) * 100
		positive_q = int(np.sum(self.q_table > 0))
		positive_ratio = (positive_q / (num_states * self.action_dim)) * 100
		avg_q = float(np.mean(self.q_table))
		
		# State visitation statistics
		visited_states = int(np.count_nonzero(self.state_visits))
		state_coverage = (visited_states / num_states) * 100
		min_visits = int(np.min(self.state_visits[self.state_visits > 0])) if visited_states > 0 else 0
		max_visits = int(np.max(self.state_visits))
		
		# Action distribution statistics
		action_entropy = -np.sum((self.action_counts / np.sum(self.action_counts)) * 
								np.log(self.action_counts / np.sum(self.action_counts) + 1e-10))
		
		return {
			"state_dimensions": self.state_dims,
			"action_dimension": self.action_dim,
			"total_state_action_pairs": num_states * self.action_dim,
			"nonzero_q_values": num_nonzero,
			"coverage_percentage": coverage,
			"positive_q_values": positive_q,
			"positive_q_percentage": positive_ratio,
			"average_q_value": avg_q,
			"training_episodes": self.training_episodes,
			"training_steps": self.training_steps,
			"visited_states": visited_states,
			"state_coverage_percentage": state_coverage,
			"min_state_visits": min_visits,
			"max_state_visits": max_visits,
			"action_entropy": float(action_entropy),
			"action_distribution": self.action_counts.tolist(),
		}

	def get_underexplored_states(self, threshold: int = 5) -> List[Tuple[int, ...]]:
		"""Get states that have been visited fewer than threshold times"""
		underexplored = []
		for idx in np.ndindex(self.state_visits.shape):
			if 0 < self.state_visits[idx] < threshold:
				underexplored.append(idx)
		return underexplored

	def get_unvisited_states(self) -> List[Tuple[int, ...]]:
		"""Get states that have never been visited"""
		unvisited = []
		for idx in np.ndindex(self.state_visits.shape):
			if self.state_visits[idx] == 0:
				unvisited.append(idx)
		return unvisited

__all__ = ["QLearningAgent"]

# ===============================================================================
# FILE: ddos_rl/config.py
# ===============================================================================

"""Minimal config for ddos_rl to be self-contained."""

BATTERY_SPECS = {
    "VOLTAGE": 22.2,          # 6S LiPo
    "CAPACITY_MAH": 5200,     # mAh
    "CAPACITY_WH": 22.2 * 5.2 # Watt-hours (~115.44 Wh)
}

__all__ = ["BATTERY_SPECS"]

# ===============================================================================
# FILE: ddos_rl/env.py
# ===============================================================================

"""
TacticalUAVEnv (moved from top-level tactical_simulator.py)
"""

import numpy as np
try:
	import gym
	from gym import spaces
except ImportError:
	import gymnasium as gym
	from gymnasium import spaces

# Battery specifications - CORRECTED to match context.txt exact hardware
# "Pro Range Lipo, 6 Cells, 22.2 V, 35C" from context.txt
BATTERY_SPECS = {
    "VOLTAGE": 22.2,          # 6S LiPo (6 cells)
    "CAPACITY_MAH": 5200,     # mAh (standard for Pro Range)
    "CAPACITY_WH": 115.44,    # 22.2V * 5.2Ah = 115.44 Wh
    "C_RATING": 35,           # 35C discharge rate
    "MAX_DISCHARGE_A": 182,   # 5.2Ah * 35C = 182A max discharge
    "CELL_COUNT": 6,          # 6S configuration
    "CELL_VOLTAGE_NOMINAL": 3.7,  # 3.7V per cell nominal
    "CELL_VOLTAGE_MAX": 4.2,      # 4.2V per cell fully charged
    "CELL_VOLTAGE_MIN": 3.0,      # 3.0V per cell minimum safe
}

from .profiles import (
	get_power_consumption_robust, get_ddos_execution_time_robust,
	get_security_rating, CPU_FREQUENCY_PRESETS,
)


class TacticalUAVEnv(gym.Env):
	"""
	A simulator for the tactical UAV environment.
	This environment simulates a single drone making decisions about DDoS detection
	and CPU management to optimize security, performance, and battery life.
	"""

	def __init__(self):
		super(TacticalUAVEnv, self).__init__()

		# Actions: 2 models x 4 CPU presets + 1 de-escalate = 9
		# Mapping:
		# 0..3: XGBOOST @ [POWERSAVE, BALANCED, PERFORMANCE, TURBO]
		# 4..7: TST     @ [POWERSAVE, BALANCED, PERFORMANCE, TURBO]
		# 8:    DE-ESCALATE (no DDoS scanning)
		self.action_space = spaces.Discrete(9)

		# State space: Threat(4), Battery(4), CPU Load(3), Task Priority(3)
		self.observation_space = spaces.MultiDiscrete([4, 4, 3, 3])

		# Time and battery model
		self.time_step = 5  # seconds per step
		self.max_steps = 500
		self.capacity_Wh = BATTERY_SPECS["CAPACITY_WH"]

		# Initialize episode
		self.reset()

	def _get_state(self):
		return [
			self.threat_level_idx,
			self.battery_state_idx,
			self.cpu_load_idx,
			self.task_priority_idx,
		]

	def reset(self):
		# Battery and env state
		self.battery_percentage = 100.0
		self.threat_level_idx = 0  # NONE
		self.battery_state_idx = 3  # HIGH
		self.cpu_load_idx = 1  # NORMAL
		self.task_priority_idx = 1  # MEDIUM
		self.steps = 0

		# Current configuration
		self._cpu_keys = list(CPU_FREQUENCY_PRESETS.keys())
		self.current_model_idx = 0  # XGBOOST
		self.current_freq_idx = 1  # BALANCED
		self.default_cores = 2     # Use 2 cores by default for profiles
		self.cpu_frequency = CPU_FREQUENCY_PRESETS[self._cpu_keys[self.current_freq_idx]]
		self.active_cores = self.default_cores
		return self._get_state()

	def _update_battery_state(self):
		p = self.battery_percentage
		if p < 20:
			self.battery_state_idx = 0  # CRITICAL
		elif p < 50:
			self.battery_state_idx = 1  # LOW
		elif p < 80:
			self.battery_state_idx = 2  # MEDIUM
		else:
			self.battery_state_idx = 3  # HIGH

	def _update_environment(self):
		# Threat drift
		if np.random.random() < 0.1:
			change = np.random.choice([-1, 0, 1], p=[0.2, 0.6, 0.2])
			self.threat_level_idx = int(np.clip(self.threat_level_idx + change, 0, 3))
		# CPU load derived from preset intensity
		base_load = np.random.normal(0.5, 0.2)
		freq_factor = 1.0 - (self.current_freq_idx / 6.0)
		adjusted = base_load * freq_factor
		if adjusted < 0.3:
			self.cpu_load_idx = 0
		elif adjusted < 0.7:
			self.cpu_load_idx = 1
		else:
			self.cpu_load_idx = 2
		# Task priority occasional change
		if np.random.random() < 0.05:
			self.task_priority_idx = int(np.random.choice([0, 1, 2]))

	def _calculate_reward(self, model_idx, freq_idx, deescalate: bool):
		reward = 0.0
		reward_breakdown = {}
		
		if deescalate:
			# No scanning: low power is good on low threat/low battery; risky on high threat
			threat_penalty = 0
			battery_bonus = 0
			if self.threat_level_idx >= 2:
				threat_penalty = -25  # under-monitoring under high threat
				reward += threat_penalty
			if self.battery_state_idx <= 1:
				battery_bonus = 8   # preserving battery when low
				reward += battery_bonus
			
			reward_breakdown = {
				"threat_penalty": threat_penalty,
				"battery_bonus": battery_bonus,
				"total": reward
			}
			return reward, reward_breakdown

		model = ["XGBOOST", "TST"][model_idx]
		cpu_key = self._cpu_keys[freq_idx]
		cpu_frequency = CPU_FREQUENCY_PRESETS[cpu_key]
		active_cores = self.default_cores

		power_watts = get_power_consumption_robust(cpu_frequency, active_cores)
		execution_time = get_ddos_execution_time_robust(model, cpu_frequency, active_cores)
		security = get_security_rating(model)

		# Energy-conscious term
		energy_penalty = 0
		if self.battery_state_idx <= 1:
			energy_penalty = power_watts * 2.0
		else:
			energy_penalty = power_watts * 0.5
		reward -= energy_penalty

		# Performance term (smooth penalty)
		execution_penalty = np.clip(execution_time - 1.0, 0, 4) * 5
		reward -= execution_penalty
		performance_bonus = 5 if execution_time <= 1.0 else 0
		reward += performance_bonus

		# Security term
		security_reward = 0
		if self.threat_level_idx >= 2:
			security_reward = 20 if security >= 8 else -15
		else:
			if security >= 4:
				security_reward = 5
		reward += security_reward

		# CPU load scaling
		cpu_bonus = 10 if (self.cpu_load_idx == 2 and freq_idx >= 2) else 0
		reward += cpu_bonus

		# Task priority scaling
		priority_bonus = 15 if (self.task_priority_idx == 0 and freq_idx >= 2) else 0
		reward += priority_bonus

		reward_breakdown = {
			"energy_penalty": -energy_penalty,
			"execution_penalty": -execution_penalty,
			"performance_bonus": performance_bonus,
			"security_reward": security_reward,
			"cpu_bonus": cpu_bonus,
			"priority_bonus": priority_bonus,
			"total": reward
		}

		return reward, reward_breakdown

	def step(self, action):
		assert self.action_space.contains(action), f"Invalid action: {action}"
		# Decode: 0..7 are (model,freq), 8 is de-escalate
		if action == 8:
			deescalate = True
			model_idx = None
			freq_idx = 0  # use POWERSAVE for power computation
		else:
			deescalate = False
			model_idx = action // 4
			freq_idx = action % 4

		# Apply configuration
		self.current_model_idx = model_idx if model_idx is not None else self.current_model_idx
		self.current_freq_idx = freq_idx
		self._cpu_keys = list(CPU_FREQUENCY_PRESETS.keys())
		cpu_key = self._cpu_keys[freq_idx]
		self.cpu_frequency = CPU_FREQUENCY_PRESETS[cpu_key]
		self.active_cores = self.default_cores

		reward, reward_breakdown = self._calculate_reward(model_idx, freq_idx, deescalate)
		power_watts = get_power_consumption_robust(self.cpu_frequency, self.active_cores)

		# Battery drain via Wh model
		energy_Wh = power_watts * (self.time_step / 3600.0)
		pct_drain = (energy_Wh / self.capacity_Wh) * 100.0
		self.battery_percentage -= pct_drain

		self._update_battery_state()
		self._update_environment()
		self.steps += 1

		done = False
		if self.battery_percentage <= 0 or self.steps >= self.max_steps:
			done = True
			if self.battery_percentage <= 0:
				reward -= 50

		state = self._get_state()
		info = {
			"battery_percentage": self.battery_percentage,
			"model": (None if deescalate else ["XGBOOST", "TST"][self.current_model_idx]),
			"cpu_frequency": self.cpu_frequency,
			"active_cores": self.active_cores,
			"power_watts": power_watts,
			"energy_Wh": energy_Wh,
			"battery_capacity_Wh": self.capacity_Wh,
			"reward": reward,
			"reward_breakdown": reward_breakdown,
		}
		return state, reward, done, info

	def render(self, mode='human'):
		threat_levels = ["NONE", "POTENTIAL", "CONFIRMING", "CRITICAL"]
		battery_states = ["CRITICAL", "LOW", "MEDIUM", "HIGH"]
		cpu_loads = ["LOW", "NORMAL", "HIGH"]
		task_priorities = ["CRITICAL", "HIGH", "MEDIUM"]
		models = ["XGBOOST", "TST"]
		frequencies = list(CPU_FREQUENCY_PRESETS.keys())

		print(f"Step: {self.steps}/{self.max_steps}")
		print(f"Battery: {self.battery_percentage:.1f}% ({battery_states[self.battery_state_idx]})")
		print(f"Threat Level: {threat_levels[self.threat_level_idx]}")
		print(f"CPU Load: {cpu_loads[self.cpu_load_idx]}")
		print(f"Task Priority: {task_priorities[self.task_priority_idx]}")
		model_str = "DE-ESCALATED" if self.current_model_idx is None else models[self.current_model_idx]
		print(f"Current Model: {model_str}")
		print(f"CPU Frequency: {frequencies[self.current_freq_idx]} ({self.cpu_frequency} MHz)")
		print(f"Active Cores: {self.active_cores}")
		print(f"Power Consumption: {get_power_consumption_robust(self.cpu_frequency, self.active_cores):.2f}W")
		print("-" * 30)

__all__ = ["TacticalUAVEnv"]

# ===============================================================================
# FILE: ddos_rl/profiles.py
# ===============================================================================

"""
Performance profiles for DDoS detection and power consumption.
Based on empirical data from context.txt.
"""

import numpy as np

# CPU frequency presets (MHz) - from context.txt hardware specs
CPU_FREQUENCY_PRESETS = {
    "POWERSAVE": 600,    # Minimum frequency for power saving
    "BALANCED": 1200,    # Balanced performance/power
    "PERFORMANCE": 1800, # High performance
    "TURBO": 2400,       # Maximum turbo frequency
}

def get_power_consumption_robust(cpu_frequency: int, active_cores: int) -> float:
    """
    Get power consumption based on CPU frequency and active cores.
    Returns power in Watts.
    """
    # Base power consumption (idle system)
    base_power = 2.5  # Watts for RPi 4B idle
    
    # CPU power scaling with frequency and cores
    freq_factor = cpu_frequency / 1200.0  # Normalized to BALANCED
    core_factor = active_cores / 4.0       # Normalized to max cores
    
    # Dynamic power consumption
    cpu_power = 3.0 * freq_factor * freq_factor * core_factor
    
    total_power = base_power + cpu_power
    return np.clip(total_power, 2.5, 12.0)  # Realistic bounds for RPi 4B

def get_ddos_execution_time_robust(model: str, cpu_frequency: int, active_cores: int) -> float:
    """
    Get DDoS detection execution time based on model, CPU frequency, and cores.
    Returns time in seconds.
    """
    # Base execution times (at BALANCED frequency, 2 cores)
    base_times = {
        "XGBOOST": 0.8,  # Faster, lighter model
        "TST": 1.2,      # Slower, more complex model
    }
    
    if model not in base_times:
        return 2.0  # Default for unknown models
    
    base_time = base_times[model]
    
    # Scaling factors
    freq_factor = 1200.0 / cpu_frequency  # Inverse relationship
    core_factor = 2.0 / active_cores       # Inverse relationship (parallelization)
    
    # Apply scaling with realistic bounds
    execution_time = base_time * freq_factor * core_factor
    return np.clip(execution_time, 0.2, 5.0)  # Realistic bounds

def get_security_rating(model: str) -> int:
    """
    Get security rating for DDoS detection model.
    Returns rating from 1-10 (higher is better).
    """
    ratings = {
        "XGBOOST": 7,  # Good general performance
        "TST": 9,      # Better for complex patterns
    }
    return ratings.get(model, 5)  # Default rating

__all__ = [
    "CPU_FREQUENCY_PRESETS",
    "get_power_consumption_robust",
    "get_ddos_execution_time_robust", 
    "get_security_rating"
]

# ===============================================================================
# FILE: ddos_rl/scheduler_demo.py
# ===============================================================================

# Thin re-export wrapper to keep imports stable
from UAVScheduler import UAVScheduler
__all__ = ["UAVScheduler"]

# ===============================================================================
# FILE: ddos_rl/train.py
# ===============================================================================

"""
Training utilities (moved from top-level train_tactical.py)
"""

import os
import sys
import numpy as np
import matplotlib.pyplot as plt
from tqdm import tqdm
import time

# Add utils to path for reproducibility
sys.path.append(os.path.join(os.path.dirname(__file__), '..'))
from utils.reproducibility import setup_tactical_training_metadata, add_model_hashes

from .env import TacticalUAVEnv
from .agent import QLearningAgent


def train_tactical_agent(episodes=10000, eval_frequency=100, output_dir="output", seed: int | None = 123, checkpoint_every: int = 500):
	"""Train the tactical (UAV-side) Q-learning agent.

	Adds:
	  - Deterministic seeding
	  - CSV logging of rewards / eval scores
	  - Periodic checkpointing
	"""
	if seed is not None:
		np.random.seed(seed)

	# Setup reproducibility metadata
	metadata = setup_tactical_training_metadata(episodes, seed, output_dir)
	print(f"Training metadata saved to {output_dir}/run_metadata.json")

	env = TacticalUAVEnv()
	state_dims = [4, 4, 3, 3]
	action_dim = env.action_space.n
	agent = QLearningAgent(
		state_dims=state_dims,
		action_dim=action_dim,
		learning_rate=0.1,
		discount_factor=0.99,
		exploration_rate=1.0,
		exploration_decay=0.995,
		min_exploration_rate=0.01,
	)

	all_episode_rewards: list[float] = []
	evaluation_rewards: list[float] = []
	epsilon_values: list[float] = []
	best_eval_reward = -float('inf')

	os.makedirs(output_dir, exist_ok=True)
	log_csv = os.path.join(output_dir, "tactical_training_log.csv")
	with open(log_csv, "w", encoding="utf-8") as f:
		f.write("episode,reward,eval_reward,epsilon,reward_breakdown\n")

	start_time = time.time()
	for episode in tqdm(range(episodes), desc="Training Tactical Agent"):
		state = env.reset()
		done = False
		episode_reward = 0.0
		while not done:
			action = agent.choose_action(state)
			next_state, reward, done, _ = env.step(action)
			agent.learn(state, action, reward, next_state, done)
			state = next_state
			episode_reward += reward
		all_episode_rewards.append(episode_reward)
		epsilon_values.append(agent.epsilon)
		agent.training_episodes += 1

		eval_reward_str = ""
		if (episode + 1) % eval_frequency == 0:
			eval_reward = evaluate_agent(agent, env, episodes=10)
			evaluation_rewards.append(eval_reward)
			eval_reward_str = f"{eval_reward:.4f}"
			if eval_reward > best_eval_reward:
				best_eval_reward = eval_reward
				agent.save_policy(f"{output_dir}/tactical_q_table_best.npy")
			print(
				f"Episode {episode+1}/{episodes} | EpReward {episode_reward:.2f} | Eval {eval_reward:.2f} | Eps {agent.epsilon:.4f}"
			)

		# Periodic checkpoint independent of eval frequency
		if (episode + 1) % checkpoint_every == 0:
			agent.save_policy(f"{output_dir}/tactical_q_table_ckpt_{episode+1}.npy")

		with open(log_csv, "a", encoding="utf-8") as f:
			f.write(f"{episode+1},{episode_reward:.4f},{eval_reward_str},{agent.epsilon:.6f}\n")

	training_time = time.time() - start_time
	print(f"Training completed in {training_time:.2f} seconds")
	agent.save_policy(f"{output_dir}/tactical_q_table.npy")
	
	# Add model hashes to metadata
	model_files = {
		"tactical_q_table": f"{output_dir}/tactical_q_table.npy",
		"tactical_q_table_best": f"{output_dir}/tactical_q_table_best.npy"
	}
	add_model_hashes(f"{output_dir}/run_metadata.json", model_files)
	
	plot_training_curves(all_episode_rewards, evaluation_rewards, epsilon_values, output_dir)
	return agent


def evaluate_agent(agent, env, episodes=10):
	total_reward = 0
	for _ in range(episodes):
		state = env.reset()
		done = False
		episode_reward = 0
		while not done:
			action = agent.choose_action(state, training=False)
			next_state, reward, done, _ = env.step(action)
			state = next_state
			episode_reward += reward
		total_reward += episode_reward
	return total_reward / episodes


def plot_training_curves(rewards, eval_rewards, epsilons, output_dir):
	plt.figure(figsize=(15, 15))
	plt.subplot(3, 1, 1)
	plt.plot(rewards)
	plt.title('Episode Rewards')
	plt.xlabel('Episode')
	plt.ylabel('Reward')
	plt.subplot(3, 1, 2)
	window_size = 100
	smoothed_rewards = np.convolve(rewards, np.ones(window_size)/window_size, mode='valid')
	plt.plot(smoothed_rewards)
	plt.title(f'Smoothed Rewards (Window Size: {window_size})')
	plt.xlabel('Episode')
	plt.ylabel('Smoothed Reward')
	plt.subplot(3, 1, 3)
	if eval_rewards:
		eval_episodes = np.arange(0, len(rewards), max(1, len(rewards)//max(1,len(eval_rewards))))[:len(eval_rewards)]
		plt.plot(eval_episodes, eval_rewards, 'r-')
	plt.title('Evaluation Rewards')
	plt.xlabel('Episode')
	plt.ylabel('Evaluation Reward')
	plt.tight_layout()
	plt.savefig(f"{output_dir}/tactical_training_curves.png")
	plt.close()
	plt.figure(figsize=(10, 5))
	plt.plot(epsilons)
	plt.title('Exploration Rate (Epsilon)')
	plt.xlabel('Episode')
	plt.ylabel('Epsilon')
	plt.savefig(f"{output_dir}/tactical_epsilon.png")
	plt.close()

# ===============================================================================
# FILE: thermal_aware_tactical_rl.py - THERMAL AND RESOURCE MONITORING ENHANCEMENTS
# ===============================================================================

"""
Thermal-Aware Tactical RL Environment for UAV DDoS Detection
Integrates RPi temperature monitoring, resource utilization, and XGBoost failure handling
"""

import numpy as np
import psutil
import time
from typing import Dict, List, Tuple, Optional
from dataclasses import dataclass
from enum import Enum

class ThermalState(Enum):
    OPTIMAL = 0      # < 60°C
    WARM = 1         # 60-70°C  
    HOT = 2          # 70-80°C
    CRITICAL = 3     # > 80°C

class DetectionReliability(Enum):
    EXCELLENT = 0    # > 95% success rate
    GOOD = 1         # 80-95% success rate
    POOR = 2         # 60-80% success rate
    FAILING = 3      # < 60% success rate

@dataclass
class SystemResources:
    cpu_utilization: float      # 0-100%
    memory_utilization: float   # 0-100%
    temperature: float          # Celsius
    thermal_state: ThermalState
    available_cores: int
    current_frequency: int      # MHz
    battery_voltage: float      # Volts
    power_consumption: float    # Watts

class ThermalAwareTacticalUAVEnv:
    """
    Enhanced Tactical UAV Environment with thermal awareness and resource monitoring.
    Extends the original TacticalUAVEnv with advanced features for real-world deployment.
    """
    
    def __init__(self, drone_id: str = "tactical_drone"):
        self.drone_id = drone_id
        
        # Enhanced state space: [Threat, Battery, CPU_Load, Task_Priority, ThermalState, ResourceUsage, DetectionReliability, ConnectionStatus]
        self.state_dims = [4, 4, 3, 3, 4, 4, 4, 2]  # 6,144 states
        self.action_space_size = 9  # Keep original 9 actions (2 models × 4 CPU + de-escalate)
        
        # Thermal management
        self.thermal_history = []
        self.thermal_throttle_active = False
        self.overheating_events = 0
        
        # XGBoost failure tracking
        self.xgboost_failure_count = 0
        self.tst_failure_count = 0
        self.detection_success_history = []
        self.fallback_detection_active = False
        
        # Resource monitoring
        self.resource_history = []
        self.cpu_throttling_active = False
        
        # Anti-greedy exploration
        self.state_visit_counts = np.zeros(self.state_dims)
        self.exploration_bonus = 0.1
        self.curiosity_threshold = 15
        
        # Battery and power management
        self.battery_critical_threshold = 0.2
        self.power_save_mode = False
        
        # Connection and swarm awareness
        self.connection_status = True
        self.swarm_alert_level = 0  # 0-3
        
    def get_system_resources(self) -> SystemResources:
        """Get current system resource utilization and thermal state."""
        
        # CPU utilization
        cpu_percent = psutil.cpu_percent(interval=0.1)
        
        # Memory utilization  
        memory = psutil.virtual_memory()
        memory_percent = memory.percent
        
        # Temperature monitoring
        temperature = self._get_cpu_temperature()
        thermal_state = self._classify_thermal_state(temperature)
        
        # Available cores and frequency
        available_cores = psutil.cpu_count(logical=False)
        current_freq = self._get_current_cpu_frequency()
        
        # Battery and power (simulated for now)
        battery_voltage = self._get_battery_voltage()
        power_consumption = self._estimate_power_consumption(cpu_percent, current_freq)
        
        return SystemResources(
            cpu_utilization=cpu_percent,
            memory_utilization=memory_percent,
            temperature=temperature,
            thermal_state=thermal_state,
            available_cores=available_cores,
            current_frequency=current_freq,
            battery_voltage=battery_voltage,
            power_consumption=power_consumption
        )
    
    def _get_cpu_temperature(self) -> float:
        """Get RPi CPU temperature with fallback."""
        try:
            import subprocess
            result = subprocess.run(['vcgencmd', 'measure_temp'], 
                                  capture_output=True, text=True)
            temp_str = result.stdout.strip()
            temp = float(temp_str.split('=')[1].split("'")[0])
            return temp
        except:
            # Fallback: estimate from CPU usage and time
            cpu_percent = psutil.cpu_percent()
            base_temp = 45.0
            load_temp = cpu_percent * 0.4
            time_factor = (time.time() % 3600) / 3600 * 5  # Simulate thermal cycling
            return base_temp + load_temp + time_factor
    
    def _classify_thermal_state(self, temperature: float) -> ThermalState:
        """Classify thermal state based on temperature."""
        if temperature < 60:
            return ThermalState.OPTIMAL
        elif temperature < 70:
            return ThermalState.WARM
        elif temperature < 80:
            return ThermalState.HOT
        else:
            return ThermalState.CRITICAL
    
    def _get_current_cpu_frequency(self) -> int:
        """Get current CPU frequency in MHz."""
        try:
            freq_info = psutil.cpu_freq()
            return int(freq_info.current) if freq_info else 1200
        except:
            return 1200
    
    def _get_battery_voltage(self) -> float:
        """Get battery voltage (simulated)."""
        # Simulate battery discharge over time
        base_voltage = 22.2  # 6S LiPo nominal
        discharge_factor = 0.95 + 0.05 * np.sin(time.time() / 1000)  # Simulate discharge
        return base_voltage * discharge_factor
    
    def _estimate_power_consumption(self, cpu_percent: float, frequency: int) -> float:
        """Estimate power consumption based on CPU usage and frequency."""
        # Base power consumption model from empirical data
        base_power = 3.5  # Base RPi power
        cpu_power = (cpu_percent / 100) * (frequency / 1000) * 2.0  # CPU scaling
        return base_power + cpu_power
    
    def get_enhanced_state(self) -> List[int]:
        """
        Get enhanced state vector including thermal and resource information.
        
        Returns:
            List of discrete state values for RL agent
        """
        # Get base tactical state
        threat_level = self._get_current_threat_level()
        battery_level = self._get_battery_level_discrete()
        cpu_load = self._get_cpu_load_discrete()
        task_priority = self._get_task_priority()
        
        # Get system resources
        resources = self.get_system_resources()
        thermal_state = resources.thermal_state.value
        resource_usage = self._discretize_resource_usage(resources)
        
        # Get detection reliability
        detection_reliability = self._get_detection_reliability_state()
        
        # Get connection status
        connection_status = 1 if self.connection_status else 0
        
        state = [
            threat_level,
            battery_level, 
            cpu_load,
            task_priority,
            thermal_state,
            resource_usage,
            detection_reliability,
            connection_status
        ]
        
        # Update state visit counts for anti-greedy exploration
        try:
            state_tuple = tuple(state)
            if len(state_tuple) == len(self.state_dims):
                self.state_visit_counts[state_tuple] += 1
        except (IndexError, TypeError):
            pass
        
        return state
    
    def _discretize_resource_usage(self, resources: SystemResources) -> int:
        """Convert resource usage to discrete state."""
        # Combine CPU, memory, and thermal load
        cpu_load = resources.cpu_utilization / 100
        memory_load = resources.memory_utilization / 100
        thermal_load = min(1.0, (resources.temperature - 40) / 40)  # Normalize 40-80°C to 0-1
        
        combined_load = (cpu_load + memory_load + thermal_load) / 3
        
        if combined_load < 0.25:
            return 0  # LOW
        elif combined_load < 0.50:
            return 1  # MEDIUM
        elif combined_load < 0.75:
            return 2  # HIGH
        else:
            return 3  # CRITICAL
    
    def _get_detection_reliability_state(self) -> int:
        """Get detection reliability state based on recent performance."""
        if len(self.detection_success_history) < 5:
            return DetectionReliability.GOOD.value
        
        recent_success_rate = sum(self.detection_success_history[-10:]) / min(10, len(self.detection_success_history))
        
        if recent_success_rate > 0.95:
            return DetectionReliability.EXCELLENT.value
        elif recent_success_rate > 0.80:
            return DetectionReliability.GOOD.value
        elif recent_success_rate > 0.60:
            return DetectionReliability.POOR.value
        else:
            return DetectionReliability.FAILING.value
    
    def calculate_thermal_aware_reward(self, state: List[int], action: int, 
                                     next_state: List[int], info: Dict) -> float:
        """
        Calculate reward with thermal awareness and resource monitoring.
        """
        base_reward = 0
        resources = self.get_system_resources()
        
        # 1. Thermal management reward (30%)
        thermal_reward = self._calculate_thermal_reward(resources, action)
        base_reward += 0.30 * thermal_reward
        
        # 2. Resource efficiency reward (20%)
        resource_reward = self._calculate_resource_efficiency_reward(resources, action)
        base_reward += 0.20 * resource_reward
        
        # 3. Detection reliability reward (20%)
        detection_reward = self._calculate_detection_reliability_reward(action, info)
        base_reward += 0.20 * detection_reward
        
        # 4. Anti-greedy exploration bonus (10%)
        exploration_reward = self._calculate_exploration_bonus(state)
        base_reward += 0.10 * exploration_reward
        
        # 5. Base tactical reward (20%)
        base_tactical_reward = self._calculate_base_tactical_reward(state, action, next_state, info)
        base_reward += 0.20 * base_tactical_reward
        
        # Critical safety constraints
        safety_penalty = self._calculate_safety_penalties(resources, action)
        base_reward += safety_penalty
        
        return base_reward
    
    def _calculate_thermal_reward(self, resources: SystemResources, action: int) -> float:
        """Calculate thermal-based reward/penalty."""
        thermal_state = resources.thermal_state
        action_type = self._classify_action_thermal_impact(action)
        
        if thermal_state == ThermalState.CRITICAL:
            self.thermal_throttle_active = True
            self.overheating_events += 1
            
            if action_type == "HIGH_POWER":
                return -100.0  # Massive penalty for high-power actions when critical
            elif action_type == "MEDIUM_POWER":
                return -50.0
            else:  # LOW_POWER or DE_ESCALATE
                return 10.0  # Reward for appropriate thermal response
                
        elif thermal_state == ThermalState.HOT:
            if action_type == "HIGH_POWER":
                return -25.0  # Penalty for high-power when hot
            elif action_type == "MEDIUM_POWER":
                return -5.0
            else:
                return 15.0  # Bonus for thermal-aware decisions
                
        elif thermal_state == ThermalState.WARM:
            if action_type == "HIGH_POWER":
                return -5.0  # Small penalty
            else:
                return 5.0  # Small bonus
                
        else:  # OPTIMAL
            self.thermal_throttle_active = False
            if action_type == "HIGH_POWER":
                return 10.0  # Can use high power when thermal conditions are good
            else:
                return 5.0  # Bonus for optimal thermal state
    
    def _calculate_resource_efficiency_reward(self, resources: SystemResources, action: int) -> float:
        """Calculate resource efficiency reward."""
        cpu_util = resources.cpu_utilization
        memory_util = resources.memory_utilization
        power_consumption = resources.power_consumption
        
        # Reward efficient resource usage
        efficiency_score = 0
        
        # CPU efficiency
        if cpu_util < 30:
            efficiency_score += 10  # Good CPU efficiency
        elif cpu_util < 70:
            efficiency_score += 5   # Moderate efficiency
        else:
            efficiency_score -= 5   # High CPU usage penalty
        
        # Memory efficiency
        if memory_util < 50:
            efficiency_score += 5
        elif memory_util > 80:
            efficiency_score -= 10  # Memory pressure penalty
        
        # Power efficiency (based on empirical data)
        if power_consumption < 5.0:
            efficiency_score += 10  # Excellent power efficiency
        elif power_consumption < 7.0:
            efficiency_score += 5   # Good efficiency
        else:
            efficiency_score -= 5   # High power consumption
        
        return efficiency_score
    
    def _calculate_detection_reliability_reward(self, action: int, info: Dict) -> float:
        """Calculate detection reliability reward based on method performance."""
        detection_method = self._extract_detection_method_from_action(action)
        
        # Track detection success
        detection_success = info.get("detection_success", True)
        self.detection_success_history.append(1 if detection_success else 0)
        
        # Keep only recent history
        if len(self.detection_success_history) > 50:
            self.detection_success_history = self.detection_success_history[-50:]
        
        # Handle method-specific failures
        if not detection_success:
            if detection_method == "XGBOOST":
                self.xgboost_failure_count += 1
                if self.xgboost_failure_count >= 3:
                    self.fallback_detection_active = True
                    return -20.0  # Penalty for continued XGBoost failures
            elif detection_method == "TST":
                self.tst_failure_count += 1
                return -15.0  # Penalty for TST failure
        else:
            # Reset failure counts on success
            if detection_method == "XGBOOST":
                self.xgboost_failure_count = max(0, self.xgboost_failure_count - 1)
            elif detection_method == "TST":
                self.tst_failure_count = max(0, self.tst_failure_count - 1)
        
        # Reward method diversity when one method is failing
        if self.fallback_detection_active and detection_method == "TST":
            return 25.0  # Strong reward for using TST when XGBoost is unreliable
        
        # Base detection reward
        return 10.0 if detection_success else -10.0
    
    def _calculate_exploration_bonus(self, state: List[int]) -> float:
        """Anti-greedy exploration bonus for less-visited states."""
        try:
            state_tuple = tuple(state)
            visit_count = self.state_visit_counts[state_tuple]
            if visit_count < self.curiosity_threshold:
                return self.exploration_bonus * (self.curiosity_threshold - visit_count)
        except (IndexError, TypeError):
            return self.exploration_bonus * self.curiosity_threshold
        
        return 0.0
    
    def _calculate_safety_penalties(self, resources: SystemResources, action: int) -> float:
        """Apply critical safety constraints."""
        penalty = 0.0
        
        # Thermal safety - Never high-power actions when overheating
        if resources.thermal_state == ThermalState.CRITICAL:
            action_type = self._classify_action_thermal_impact(action)
            if action_type in ["HIGH_POWER", "MEDIUM_POWER"]:
                penalty -= 200.0  # Massive penalty for unsafe thermal actions
        
        # Battery safety - Force power-saving when battery critical
        battery_level = self._get_battery_level_discrete()
        if battery_level == 0:  # Critical battery
            action_type = self._classify_action_thermal_impact(action)
            if action_type == "HIGH_POWER":
                penalty -= 150.0  # Force power conservation
        
        # Resource safety - Prevent system overload
        if resources.cpu_utilization > 90 or resources.memory_utilization > 90:
            action_type = self._classify_action_thermal_impact(action)
            if action_type == "HIGH_POWER":
                penalty -= 100.0  # Prevent system overload
        
        return penalty
    
    def _classify_action_thermal_impact(self, action: int) -> str:
        """Classify action based on thermal/power impact."""
        # Based on original action space: 2 models × 4 CPU frequencies + de-escalate
        if action == 8:  # DE_ESCALATE
            return "DE_ESCALATE"
        
        cpu_freq_idx = action % 4
        if cpu_freq_idx == 0:  # 600MHz
            return "LOW_POWER"
        elif cpu_freq_idx == 1:  # 1200MHz
            return "MEDIUM_POWER"
        elif cpu_freq_idx in [2, 3]:  # 1800MHz, 2000MHz
            return "HIGH_POWER"
        
        return "MEDIUM_POWER"
    
    def _extract_detection_method_from_action(self, action: int) -> str:
        """Extract detection method from action."""
        if action == 8:  # DE_ESCALATE
            return "NONE"
        
        model_idx = action // 4
        return "XGBOOST" if model_idx == 0 else "TST"
    
    def handle_xgboost_failure(self):
        """Handle XGBoost detection failure."""
        self.xgboost_failure_count += 1
        
        if self.xgboost_failure_count >= 3:
            self.fallback_detection_active = True
            print(f"[ALERT] XGBoost failure count: {self.xgboost_failure_count}. "
                  f"Activating TST fallback detection.")
    
    def reset_detection_failures(self):
        """Reset failure counts after successful detections."""
        self.xgboost_failure_count = 0
        self.tst_failure_count = 0
        self.fallback_detection_active = False
    
    def update_swarm_state(self, swarm_alert_level: int, connection_status: bool):
        """Update swarm coordination state."""
        self.swarm_alert_level = swarm_alert_level
        self.connection_status = connection_status
    
    def get_thermal_status_summary(self) -> Dict:
        """Get thermal status summary for monitoring."""
        resources = self.get_system_resources()
        return {
            "temperature": resources.temperature,
            "thermal_state": resources.thermal_state.name,
            "thermal_throttle_active": self.thermal_throttle_active,
            "overheating_events": self.overheating_events,
            "cpu_utilization": resources.cpu_utilization,
            "memory_utilization": resources.memory_utilization,
            "power_consumption": resources.power_consumption
        }
    
    def get_detection_reliability_summary(self) -> Dict:
        """Get detection reliability summary."""
        recent_success_rate = 0.0
        if self.detection_success_history:
            recent_success_rate = sum(self.detection_success_history[-10:]) / min(10, len(self.detection_success_history))
        
        return {
            "xgboost_failure_count": self.xgboost_failure_count,
            "tst_failure_count": self.tst_failure_count,
            "fallback_detection_active": self.fallback_detection_active,
            "recent_success_rate": recent_success_rate,
            "detection_reliability_state": self._get_detection_reliability_state()
        }
    
    # Placeholder methods - implement based on existing system
    def _get_current_threat_level(self) -> int:
        return 1  # 0-3
    
    def _get_battery_level_discrete(self) -> int:
        voltage = self._get_battery_voltage()
        if voltage < 19.0:  # Critical
            return 0
        elif voltage < 20.5:  # Low
            return 1
        elif voltage < 21.5:  # Medium
            return 2
        else:  # High
            return 3
    
    def _get_cpu_load_discrete(self) -> int:
        resources = self.get_system_resources()
        cpu_util = resources.cpu_utilization
        if cpu_util < 30:
            return 0  # LOW
        elif cpu_util < 70:
            return 1  # MEDIUM
        else:
            return 2  # HIGH
    
    def _get_task_priority(self) -> int:
        # Consider swarm alert level and connection status
        if self.swarm_alert_level >= 3:  # RED alert
            return 2  # CRITICAL
        elif self.swarm_alert_level >= 2:  # ORANGE alert
            return 1  # MEDIUM
        else:
            return 0  # LOW
    
    def _calculate_base_tactical_reward(self, state: List[int], action: int, next_state: List[int], info: Dict) -> float:
        return 10.0  # Use existing tactical reward calculation

# Enhanced frequency recommendation with thermal awareness
def get_thermal_aware_cpu_frequency(temperature: float, battery_level: float, 
                                   threat_level: int, detection_method: str) -> int:
    """
    Get CPU frequency recommendation based on thermal state and operational requirements.
    
    Args:
        temperature: Current CPU temperature in Celsius
        battery_level: Battery level (0.0-1.0)
        threat_level: Threat level (0-3)
        detection_method: "XGBOOST" or "TST"
    
    Returns:
        Recommended CPU frequency in MHz
    """
    # Thermal constraints override everything
    if temperature > 80:  # CRITICAL
        return 600  # Emergency throttling
    elif temperature > 70:  # HOT
        return 1200  # Reduced frequency
    elif temperature > 60:  # WARM
        return 1500  # Moderate reduction
    
    # Battery constraints
    if battery_level < 0.2:  # Critical battery
        return 600 if threat_level <= 1 else 1200
    elif battery_level < 0.4:  # Low battery
        return 1200 if threat_level <= 2 else 1500
    
    # Threat-based frequency selection
    if threat_level >= 3:  # High threat
        return 1800  # High performance needed
    elif threat_level >= 2:  # Medium threat
        return 1500  # Balanced performance
    else:  # Low/no threat
        return 1200  # Power-efficient operation

__all__ = ["train_tactical_agent", "evaluate_agent", "ThermalAwareTacticalUAVEnv", "get_thermal_aware_cpu_frequency"]

# ===============================================================================
# DDOS-RL MODULE BACKUP COMPLETE WITH THERMAL & RESOURCE MONITORING ENHANCEMENTS
# Total Files Included: UAVScheduler.py, agent.py, config.py, env.py, profiles.py, 
# scheduler_demo.py, train.py, thermal_aware_tactical_rl.py
# 
# NEW FEATURES ADDED:
# - Thermal-aware RL with RPi temperature monitoring and thermal throttling
# - Resource monitoring (CPU utilization, memory usage, power consumption)
# - XGBoost failure handling with TST fallback detection
# - Anti-greedy exploration strategy with state visitation tracking
# - Enhanced state space (6,144 states) with thermal and resource information
# - Battery-aware frequency scaling and power management
# - Safety constraints preventing thermal damage and system overload
# - Detection reliability tracking and method diversity rewards
# ===============================================================================
