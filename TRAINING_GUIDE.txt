RL Training and Inference Guide

1) Overview
- Tactical agent (UAV): chooses DDoS model + CPU preset or de-escalate.
  • State: [Threat(4), Battery(4), CPU Load(3), Task Priority(3)]
  • Actions (9): 0..3 XGBOOST@[POWERSAVE,BALANCED,PERFORMANCE,TURBO], 4..7 TST@[...], 8 DE-ESCALATE
  • Outputs: Q-table saved to output/tactical_q_table.npy (and tactical_q_table_best.npy)
- Strategic agent (GCS): chooses crypto algo across fleet.
  • State: [SwarmThreat(3), FleetBattery(3), MissionPhase(4)]
  • Actions (4): ASCON_128, KYBER_CRYPTO, SPHINCS, FALCON512
  • Outputs: Q-table saved to output/strategic_q_table.npy

2) Environment Grounding (performance_profiles.py)
- Power (W), crypto latency (ms), and DDoS task times (s) come from empirical tables.
- CPU presets (MHz): POWERSAVE=1200, BALANCED=1500, PERFORMANCE=1800, TURBO=2000.
- Tactical simulator drains battery using Wh model from config.BATTERY_SPECS["CAPACITY_WH"].

3) How Training Works (Q-Learning)
- Epsilon-greedy policy explores then exploits.
- Update rule: Q[s,a] ← Q[s,a] + α[r + γ max_a' Q[s',a'] − Q[s,a]].
- Rewards balance power, speed, security; de-escalate trades power vs risk.

4) Running Training (Tactical)
- Script: train_tactical.py
- Saves: output/tactical_q_table.npy and plots (rewards, epsilon).
- Evaluate periodically; best policy saved to output/tactical_q_table_best.npy.

5) Running Training (Strategic)
- Script: train_strategic.py
- Saves: output/strategic_q_table.npy and plots.

6) Interpreting Outputs
- Q-table: Numpy array of shape [4,4,3,3,9] (tactical) or [3,3,4,4] (strategic); last dim is actions.
- Best action for a state: argmax along last axis.
- Tactical action mapping (index → meaning):
  0..3 = XGBOOST@[POWERSAVE,BALANCED,PERFORMANCE,TURBO]
  4..7 = TST@[POWERSAVE,BALANCED,PERFORMANCE,TURBO]
  8    = DE-ESCALATE

7) Using the Learned Policy
- Tactical: load Q-table and call agent.choose_action(state, training=False).
- Demo (UAVScheduler.py): maps action to model/preset; plug in your state provider.

8) Quick Troubleshooting
- Gym imports: the simulator supports gym or gymnasium; install one of them.
- Battery drains too fast/slow: check CAPACITY_WH and step seconds in tactical_simulator.py.
- Rewards flat or noisy: adjust reward scaling for power/latency/security; increase episodes.
- CUDA not detected: ensure correct PyTorch CUDA build and NVIDIA driver compatibility.

9) Minimal API Tips
- Tactical env state: [threat_idx, battery_idx, cpu_load_idx, task_priority_idx].
- Step returns: (next_state, reward, done, info) where info includes power_watts and energy_Wh.
- Save/load policy: rl_agent.QLearningAgent.save_policy / load_policy (.npy files).
